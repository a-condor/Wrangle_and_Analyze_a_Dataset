{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wrangle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This data wrangling project was carried out using the data wrangling process:\n",
    "1. Gather\n",
    "2. Assess\n",
    "3. Clean\n",
    "\n",
    "These 3 processes were used to gather the necessary data required; assess by identifying redundant, flawed and unwanted data present; and clean the data using the assessment. We aim at  producing a high quality data that can easily be explored and reported to gain insights and make decisions concerning this project.\n",
    "\n",
    "### GATHERING\n",
    "3 pieces of data were used in this project. Dataframes were created from:\n",
    "1. a csv file, twitter_archive \n",
    "2. a link to a tsv file, tweet_image_predictions\n",
    "3. a query using the tweepy library on Twitter API, tweet_status\n",
    "\n",
    "The dataframes extracted include  2356 rows x 17 columns, 2075 rows x 12 columns, 2326 columns x 32 rows dimensions respetively.\n",
    "\n",
    "\n",
    "### ASSESSING\n",
    "9 quality issues and 4 tidiness issues were identified by visual and programmatic assesment in these 3 dataframes. Some of these issues are however resolved together since they are related and the solution to one is sufficient to clean the other issue e.g. dropping retweets and comments.\n",
    "\n",
    "Content and structural issues identified include but are not limited to null values, missing data, incorrect value extraction, duplicates, unnecessary columns, wrong data type. These were identified using basic functions of info(), describe(), notnull(), sum(), shape.\n",
    "\n",
    "\n",
    "### CLEANING\n",
    "After making copies of each dataframes, the assessments of the issues were effected into the copied dataframes.\n",
    "1. t_archive_clean\n",
    "2. t_image_predictions_clean\n",
    "3. t_status_clean\n",
    "\n",
    "These were finally modified into dimensions of 2085 rows x 5 columns, 2075 rows x 10 columns, 2326 rows x 3 columns respectively. This was carried out using the define-code-test cleaning sequence with functions including replace(), contains(), drop(), astype(), extract(), rename(), merge(). head() and info() were often used in confirming the functionality of each code.\n",
    "\n",
    "The final copies after cleaning were merged to give a single final dataframe: twitter_archive_master with a dimension of 1951 rows x 16 columns; a big run down, but a more consistent and presentable data from the initial volume of data before assessing and cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
